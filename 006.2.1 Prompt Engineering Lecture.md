
### Temperature
Temperature: how creative you want the LLM to be
	* on a scale of 0 - 2
	* 0 = deterministic
	* 2 = completely random
	* If you don't specify temperature, the default will be 1 (non-deterministic)

### Evaluation Criteria
* Evaluation criteria: The rules or measurements we use to decide if our AI model is doing a good job.
	Examples provided to the LLM to squeeze out that last 10 - 15% of accuracy.  Give the LLM the prompt and give it the answer.
	
* Four paradigms for evaluating LLM or generative model performance (evaluation criteria):
	1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
		* The summarizer should include exact words.  
		* A *text overlap metric* that measures how similar a generated text is to a reference text. 
		* Used for *summarization* and *translation* tasks.
		* Higher ROUGE = more overlap
	2. Cosine Distance (or Cosine Similarity)
		* Where meaning matters more than the exact word match
		* Tells you how *semantically close* two texts are - even if the wording is different
		* Great for open-ended LLM tasks
	3. Custom Function
		* Create your own Python function to reflect your *specific project's success criteria*
	4. LLM as a judge
		* Let another LLM determine the accuracy of the output
		* Grades outputs based on prompts that describe what good responses should look like
		* Caveat: You have to design the evaluation prompt carefully to avoid bias and inconsistency.

### Outputs
* Outputs: 2 Types:
  1. "Fuzzy" outputs
	* Summarizer, brainstormers
	2 . Exact outputs
	* Classifiers

### Getting the LLM to return JSON
* The default output format of LLMs is markdown.  
* Things to try to get the LLM to return JSON:
	1. Adding "only return clean JSON" to both the user prompt and the system prompt
	2. Using a more updated version of the model
	3. ==Using a tool call - will 100% work==
	4. Changing the temperature to 0
	5. Add ```response_format={"type": "json_object"}```