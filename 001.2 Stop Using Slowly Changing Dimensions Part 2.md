[SCD-2 considered harmful! Part 2](https://substack.com/home/post/p-177927711)

Date stamp your data!
[Sahar Massachi](https://substack.com/@sahar) andÂ [Zach Wilson](https://substack.com/@eczachly)

Nov 04, 2025

---

Not everything you learned in college about data warehousing still applies in 2025.  
  
This is part 2 ofÂ [Saharâ€™s](https://www.linkedin.com/in/saharmassachi/)Â unlearning data warehousing concepts series. (make sure to read part 1 first if you havenâ€™tÂ _[â€œThe Data Setup No One Ever Taught Youâ€ series](https://blog.dataexpert.io/p/the-data-warehouse-setup-no-one-taught))_

Sahar and I learned a lot during out time working in core growth together at Facebook working in friending and notifications.

Letâ€™s talk about the pain of unlearning and then letâ€™s get to the magic.

Imagine youâ€™re an analyst at a social media company. The retention team asks: â€œFor users who now have 1000+ followers but had under 200 three months ago â€“ what device were they primarily using back then? And of the posts they viewed during that growth period, how many were from accounts that were mutualsÂ _at the time_?â€

You need to join user data (follower counts then and now), device data (primary device then and now), relationships (who was a mutual then vs now), and post views â€“ allÂ _as of 3 months ago._

With most data warehouse setups, this query is somewhere between â€œnightmareâ€ and â€œimpossible.â€ Â 

Youâ€™re dealing with state, not actions. State in the past, across multiple tables. Thereâ€™s a word for this problem â€“ slowly changing dimensions.Â _Whole chapters_Â of textbooks deal with various approaches. You could try logs (if you logged the right stuff). You could try slowly changing dimensions with ``valid_from`/`valid_to`` dates. You could try separate history tables. All of these approaches are painful, error-prone, and make backfilling a living hell.

Thereâ€™s a better way. Through the magic of âœ¨**datestamps**âœ¨ and idempotent pipelines, this query becomes straightforward. And backfills? They become a button you push.

[Part 1](https://blog.dataexpert.io/p/the-data-warehouse-setup-no-one-taught)Â fixed weird columns, janky tables, and trusting your SQL. Part 3 will cover scaling your team and warehouse. But now â€“ now we fix:Â [backfills](https://blog.dataexpert.io/p/how-i-made-airbnb-millions-with-this?utm_source=publication-search), 3am alerts, time complexity, data recovery, and historical queries.[1](https://substack.com/home/post/p-177927711#footnote-1-177927711)

## The old way was a mess

Hereâ€™s what most teams do when they start out:

### Option 1: Overwrite everything daily

Your pipeline runs every night, updatesÂ `dim_users`Â with todayâ€™s snapshot, overwrites yesterdayâ€™s data. Simple! Until six months later when someone asks â€œhow many followers did users have in March?â€ and you realize: that data is gone. You have no history. You canâ€™t answer the question. Oops.

_(Jargon alert â€“ Apparently this isÂ [SCD Type-1](https://medium.com/@deepakda1972/understanding-slowly-changing-dimension-scd-type-1-64b5ec571fb0)Â Â¯\_(ãƒ„)_/Â¯ )_

### Option 2: Try to track history manually

Okay, you think, letâ€™s be smarter. Add anÂ `updated_at`Â column. Or maybeÂ `valid_from`Â andÂ `valid_to`Â dates, with anÂ `is_current`Â flag. When a userâ€™s follower count changes, donâ€™t update their row â€“ instead, mark the old row as outdated and insert a new one.

_(Jargon alert â€“ This isÂ [SCD Type-2](https://medium.com/@SaiKarthikaPuttha/understanding-slowly-changing-dimension-scd-type-2-ea1563714bd7). Booo)_

This is better! You have history. But now:

- Your pipelines need custom logic to â€œclose outâ€ old rows before inserting new ones
    
- If you mess up theÂ `valid_to`Â dates, you get gaps or overlaps in history
    
- Backfilling becomes a nightmare â€“ you canâ€™t just rerun a pipeline, you need to carefully update dates without breaking everything downstream
    
- **Querying becomes a nightmare**. To get user data â€œas of 3 months agoâ€, you need:
    

`SELECT * FROM dim_users WHERE user_id = 123 AND valid_from <= â€˜2024-10-01â€™ AND (valid_to > â€˜2024-10-01â€™ OR valid_to IS NULL)`

Now imagine joining MULTIPLE historical tables (users, devices, relationships). Every join needs thatÂ `BETWEEN`Â logic. Miss one and your results are silently wrong. Get the date math slightly off and youâ€™re joining snapshots from different points in time. Good luck debugging that.

### Option 3: Separate current and history tables

Some teams maintainÂ `dim_users`Â (current snapshot) andÂ `dim_users_history`Â (everything else). Now youâ€™ve got two sources of truth to keep in sync. Analysts need to remember which table to query. Any analysis spanning current and historical data requires stitching across tables withÂ `UNION ALL.`Â Itâ€™s a mess.

And, depending on how theÂ `dim_users_history`Â table works â€“ it wonâ€™t solve any of the problems youâ€™d have in option 2!

**All of these approaches share a problem:**Â theyâ€™re trying to be clever about storage. They made sense when disk was expensive. They donâ€™t anymore.

_(Jargon alert â€“ This is SCD Type-4. Note that I didnâ€™t know this when I started writing this blog post because itâ€™sÂ **useless**,Â **boring**,Â **outdated**Â jargon. Ignore it.)_

![[SCD_Types.jpg]]

## The new way: Just append everything

You solve it with date stamps. You solve it with â€œfunctional data engineeringâ€.

What you really want is a sort of table that tracks state â€“ a dimension table â€“, but where you can access a version that tracks information about the worldÂ _today_, and another version that tracks information about the worldÂ _in the past_.

Maxime Beauchemin wrote the seminalÂ [public work on the idea here](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a). But, honestly, I think the concept can be explained more plainly and directly. So here we are.

The thinking goes like this:

- Weâ€™re getting new data all the time.
    
- Letâ€™s simplify it and say â€“ we get new data every day. We copy over snapshots from our production database each evening.
    
- There are complex, convoluted ways to keep track of what data is new and useful, and what data is a duplicate of yesterday.
    
- But wait. Storage is cheap. Compute is cheap. Pipelines can run jobs for us while we sleep.
    
- Itâ€™s annoying to have a table with the data we need as of right now, and either some specialized columns or tables to track history..
    
- Instead, what if we just kept adding data to existing tables? Add a column for â€œdate this information was trueâ€ to keep track.

Hereâ€™s what it looks like in practice. Instead of overwriting your dimension tables every day, you append to them:

```sql 
dim_users
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ user_id â”‚ followers â”‚ ds         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 123     â”‚ 150       â”‚ 2024-10-01 â”‚
â”‚ 123     â”‚ 180       â”‚ 2024-10-02 â”‚
â”‚ 123     â”‚ ...       â”‚ ...        â”‚
â”‚ 123     â”‚ 1200      â”‚ 2025-01-16 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

dim_devices
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ user_id â”‚ device  â”‚ ds         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 123     â”‚ mobile  â”‚ 2024-10-01 â”‚
â”‚ 123     â”‚ mobile  â”‚ 2024-10-02 â”‚
â”‚ 123     â”‚ ...     â”‚ ...        â”‚
â”‚ 123     â”‚ desktop â”‚ 2025-01-16 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

dim_relationships:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ user_id â”‚ friend_id â”‚ is_mutual â”‚ ds         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 123     â”‚ 789       â”‚ true      â”‚ 2024-10-01 â”‚
â”‚ 123     â”‚ 789       â”‚ true      â”‚ 2024-10-02 â”‚
â”‚ ...     â”‚ ...       â”‚ ...       â”‚ ...        â”‚
â”‚ 123     â”‚ 789       â”‚ false     â”‚ 2025-01-16 â”‚ â† changed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

fct_post_views:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ post_id â”‚ viewer_id â”‚ poster_id â”‚ ds         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5001    â”‚ 123       â”‚ 789       â”‚ 2024-10-01 â”‚
â”‚ 5002    â”‚ 123       â”‚ 456       â”‚ 2024-10-01 â”‚
â”‚ 5003    â”‚ 123       â”‚ 789       â”‚ 2024-10-05 â”‚
â”‚ ...     â”‚ ...       â”‚ ...       â”‚ ...        â”‚
â”‚ 9999    â”‚ 123       â”‚ 789       â”‚ 2025-01-15 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Now that impossible retention query becomes straightforward. NoÂ `BETWEEN`Â clauses, noÂ `valid_from`/`valid_to`Â logic â€“ just filter each table to the date you want:

```sql
-- For fast-growing users, what device did they use back then?

WITH
  today_users as (SELECT user_id, followers as today_followers
      FROM dim_users WHERE ds = â€˜2025-01-16â€™ AND followers >= 1000),
  past_users as (SELECT user_id, followers as past_followers
      FROM dim_users WHERE ds = â€˜2024-10-01â€™ AND followers < 200),
  past_device as (SELECT user_id, device
      FROM dim_devices WHERE ds = â€˜2024-10-01â€™),
  user_device as (
      SELECT tu.user_id, today_followers, past_followers, pd.device
      FROM past_users pu
      JOIN today_users tu ON pu.user_id = tu.user_id
      JOIN past_device pd ON tu.user_id = pd.user_id),
  views as (
      SELECT post_id, viewer_id, poster_id, ds
      FROM fct_post_views 
      WHERE ds BETWEEN â€˜2024-10-01â€™ AND â€˜2025-01-16â€™)
  SELECT
      ud.user_id,
      ud.device as device_during_growth,
      COUNT(DISTINCT views.post_id) as posts_from_mutuals
  FROM user_device ud
  LEFT JOIN views
      ON ud.user_id = views.viewer_id
  LEFT JOIN dim_relationships past_rels
      ON views.viewer_id = past_rels.user_id
      AND views.poster_id = past_rels.friend_id
      AND views.ds = past_rels.ds -- mutual status AS OF view date
      AND past_rels.is_mutual = true
  GROUP BY 1, 2
```

Is this query complex? Sure.[2](https://substack.com/home/post/p-177927711#footnote-2-177927711)Â But the complexity is in theÂ _business logic_Â (what youâ€™re trying to measure), not in fighting with valid_from/valid_to dates. Each query just filters to ds = {the date I want}. Thatâ€™s it.

The idea is that youâ€™re notÂ _overwriting_Â existing tables. You areÂ _appending_.Â [3](https://substack.com/home/post/p-177927711#footnote-3-177927711)

> **Sidebar: Common Table Expressions**
> 
> If I had a SECOND â€œone weird trickâ€ for data engineering, CTEs would be it. CTEs are just fucking fantastic. With liberal use of common table expressions (theÂ `WITH`Â clause you saw in the retention query above), you can treat subqueries like variables â€“ and then manipulating data feels more like code. Make sure your query engine (like Presto/Trino) flattens them for free â€“ but if it does: wowee! SQL just got dirt simple. (a free one hour course on CTEsÂ [here](https://www.youtube.com/watch?v=vstJyDo88kA))

When you grab data into your warehouse[4](https://substack.com/home/post/p-177927711#footnote-4-177927711), append a special column. That column is usually called â€œ`ds`â€ â€“ probably short for datestamp. You want something small and unobtrusive. (Notice that â€œ`date`â€ would be a bad name â€“ because youâ€™d confuse people between this (date of ingestion of data) and the more obvious sort of date â€“ date the action happened.) For snapshots, copy over the entire data of the snapshot, and have your â€œdsâ€ column be <todayâ€™s date>. For logs, you can just grab the logs since yesterday, and set the ds column to <todayâ€™s date>.

> **Sidebar: Date stamps vs Date partitions  
> **Iâ€™ll mostly say â€œdate stampsâ€ in this piece â€“ the concept of marking each row with when that data was valid/ingested.
> 
> â€œDate partitionsâ€ is how most warehouse tools *implement* date stamps. A partition is how your warehouse physically organizes data. Think of it like: all rows withÂ `ds=2025-01-15`Â get grouped together in one chunk,Â `ds=2025-01-16`Â in another chunk, and so on. (In older systems, each partition was literally a separate folder. Modern cloud warehouses abstract this, but the concept remains.)
> 
> Why does this matter? When you query ``WHERE ds=â€™2025-01-15``, your warehouse only scans that one partition instead of the entire table. This makes queries faster and cheaper (especially in cloud warehouses where you pay per data scanned).
> 
> People use the terms interchangeably. The important thing is the concept: tables with a date column that lets you query any point in history.

Every table emanating from your input tables should add a filter (`WHERE ds={today}`), and similarly append the data to the table (`WHERE ds={today}`). (Except special circumstances where a pipeline mightÂ _want_Â to look into the past).

Thatâ€™s it! Now your naive setup (overwriting everything every day) has only changed a bit (append everything each day, and keep track of what you appended when) â€“ but everything has become so much nicer.

## This is huge

This has two major implications:

First, many types of analysis become much easier. Want to know about the state of the world yesterday? Filter withÂ `WHERE ds = {yesterday}`. Need data from a month ago? Filter withÂ `WHERE ds = {a month ago}.`Â You can even mix and match â€“ comparing todayâ€™s data with historical data, all within simple queries.

Second, data engineering becomes both easier and much less error prone. You can rerun jobs, create tables with historical data, and fix bugs in the past. Your pipeline will produce consistent, fast, reliable results consistently

## What â€œfunctionalâ€ actually means

#### (Aka â€œI donâ€™t know what idempotent means and at this point Iâ€™m afraid to askâ€)

So, in Maximeâ€™s article ([link](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a)) thereâ€™s all this talk about â€œfunctional data engineeringâ€. What does that even mean? Letâ€™s discuss.

First, weâ€™re borrowing an idea from traditional programming. â€œFunctional programsâ€ (or functions) meet certain conditions:

1. If you give it the same input, you get the same output. Every time.
    
2. State doesnâ€™t change. Your inputs wonâ€™t change, hidden variables wonâ€™t change. Itâ€™s clean. (AKA â€œno side effectsâ€)
    

Okay, so what does that mean for pipelines? Functional pipelines:

- Given the same input, will give the same output
    
- Donâ€™t use (or rely on) magic secret variables
    

This is what people mean when they say â€œ[idempotent](https://www.youtube.com/live/JeeqpK3o3LQ)â€ pipelines or â€œreproducibleâ€ data.

And hereâ€™s how to implement it:Â _datestamps_.

- Your rawest/most upstream data should never be deleted â€“ just keep appending with datestamps
    
- Pipelines work the same in backfill mode vs normal daily runs
    
- If you find bugs, fix the pipeline and rerun â€“ the corrected data overwrites the bad data
    
- Time travel is built in â€“ just filter to any ds you need
    

**Datestamps also give you the nice side-effect of having it be**Â _**very clear**_Â **how fresh the data youâ€™re looking at is**. If the latest datestamp on your table is from a week ago -- itâ€™s instantly understandable not only whatâ€™s wrong, but also you have hints about why.

> **Sidebar â€“ what this looks like in practice:**  
> Your SQL will look something like:Â `WHERE ds=â€™{{ ds }}â€™`Â (Airflowâ€™s templating syntax)Â orÂ `WHERE ds=@run_date`Â (parameter binding).
> 
> Your orchestrator injects the date - whether itâ€™s todayâ€™s scheduled run or a backfill from three months ago. Same SQL, different parameter. Thatâ€™s the whole trick.

### Backfilling is now easy, simple, magical

Remember that retention query? Now imagine you built that analysis pipeline three months ago, but you just discovered a bug in yourÂ `dim_relationships`Â table. TheÂ `is_mutual`Â flag was wrong for two weeks in November. You fixed the bug going forward, but now all your retention metrics from that period are wrong.

**With the old SCD Type-2 approach, youâ€™re in hell:**

You canâ€™t just â€œrerun November.â€ Because each dayâ€™s pipeline depended on the previous dayâ€™s state. Day 15 updated rows from Day 14, which updated rows from Day 13, and so on. To fix November 15th, youâ€™d need to:

1. Rerun November 1st (building from October 31stâ€™s state)
    
2. Wait for it to finish
    
3. Rerun November 2nd (building from your new November 1st)
    
4. Wait for it to finish
    
5. Rerun November 3rd...
    
6. ...keep going for 30 days, sequentially, one at a time
    

And this is assuming nothing breaks along the way. If Day 18 fails? Start over. Need to fix December too? Add another 31 sequential runs.

![[SCD_is_bad_for_backfill.jpg]]

In Airflow terms, this is whatÂ `depends_on_past=True`Â does to you. Each day is blocked until the previous day completes.Â **Backfilling becomes painfully slow.**Â But thatâ€™s by no means the worst part.

**You canâ€™t just hit â€œbackfillâ€ and walk away.**Â Your normal daily pipeline logic doesnâ€™t work for backfills. Why? Because SCD Type-2 requires you to:

- Close out existing rows (set theirÂ `valid_to`Â date)
    
- Insert new rows (with newÂ `valid_from`Â dates)
    
- UpdateÂ `is_current`Â flags
    
- Handle the case where a row changedÂ _multiple times_Â during your backfill period
    

Your daily pipeline probably has logic like:

```sql
-- Daily SCD Type-2 pipeline (simplified)
-- Step 1: Close out changed rows
UPDATE dim_users
SET valid_to = CURRENT_DATE - 1, is_current = false
WHERE user_id IN (
SELECT user_id FROM users_source_today
WHERE <something changed>
)
AND is_current = true;

-- Step 2: Insert new versions
INSERT INTO dim_users (user_id, followers, valid_from, valid_to, is_current)
SELECT user_id, followers, CURRENT_DATE, NULL, true
FROM users_source_today;
```

This works fine when youâ€™re processing â€œtoday.â€ But for a backfill? You needÂ _different_Â SQL:

- You need to carefully reconstruct valid_from/valid_to for historical dates
    
- And handle the fact that a user might have changedÂ _multiple_Â times during your backfill window
    
- This gets messy fast.
    
- Youâ€™re essentially rewriting your pipeline. (WHY?)
    

So now youâ€™re not just waiting 30 sequential days - youâ€™re maintainingÂ _two separate codebases_: one for daily runs, one for backfills. And every time you change your daily logic, you need to update your backfill logic to match. More code to write, more code to test, more places for bugs to hide. Itâ€™s completely useless and unnecessary.

_Sidenote â€“ even worse, if youâ€™re outside your retention window (say, the source data from 90 days ago has been deleted), you canâ€™t backfill at all. Youâ€™d need to completely rebuild the entire table from scratch, from whatever historical snapshots you still have. Which probably means... datestamped snapshots anyway. Womp womp._

**With datestamps, backfilling is trivial:**

Your pipeline for any given day just needs:

- Input tables filtered toÂ `ds=â€™2024-11-15â€™`Â (or whatever day youâ€™re processing)
    
- Write output toÂ `ds=â€™2024-11-15â€™`
    

**Thatâ€™s it. November 15th doesnâ€™t need November 14th. It just needs the snapshot from November 15th.**

So to fix your broken November data:

```
# In Airflow (or whatever orchestrator)
> airflow dags backfill my_retention_pipeline \--start-date 2024-11-01 \--end-date 2024-11-30
```

What happens behind the scenes?

- All 30 days kick off inÂ _parallel_Â (up to your concurrency limits)
    
- Each day independently reads from its ds partition
    
- Each day independently writes to its ds partition
    
- No coordination needed between days
    
- The whole month finishes in the time it takes to run one day
    

**The exact same SQL that runs daily also handles backfills**Â - no special logic, no custom code

This changes everything:

**No more custom SQL for backfills**Â - Itâ€™s just a button you push. Your orchestrator handles it. The same pipeline code that runs daily also handles backfills. No special logic needed.

**New tables get history for free**Â - Created a newÂ `dim_users_enriched`Â table today but want to populate it with the last year of data? Just backfill 365 days. Since your input tables have datestamps, the data is sitting there waiting.

**Bugs in old data become fixable**Â - Fix your pipeline logic, backfill the affected date range, done. The old (wrong) data gets overwritten with the new (correct) data for those specific partitions. Everything downstream can reprocess automatically.

**Upstream changes cascade easily**Â - Fixed a bug inÂ `dim_users`? All downstream tables that depend on it can backfill the affected dates in parallel. The whole warehouse stays in sync.

This is possible because your pipelines areÂ **idempotent**. Run them once, run them a thousand times - given the same input date, you get the same output. No hidden state, no â€œcurrentâ€ vs â€œhistoricalâ€ logic, no manual date math.

**One pattern to avoid:**Â Tasks that depend on the previous dayâ€™s partition of theirÂ _own_Â table. If computing todayâ€™sÂ `dim_users`Â requires yesterdayâ€™sÂ `dim_users`, youâ€™ve created a chain - backfilling 90 days means 90 sequential runs that canâ€™t be parallelized. This is sometimesÂ [necessary for cumulative metrics](https://github.com/DataExpert-io/cumulative-table-design), but most dimension tables donâ€™t need it - just recompute from raw sources each day.

For most datestamped pipelines,Â `depends_on_past`Â should be False. Each day is independent - the only dependency is â€œdoes the upstream data exist for this ds?â€

## Welcome to the magic of easy DE work

We started this article staring at the prospect ofÂ `valid_from`/`valid_to`Â logic, sequential backfills that take days, and custom SQL for every backfill and cascading for every bugfix. Yuck. Ew!

Or maybe â€“ worse â€“ with no sense of history at all. No ability to ask â€œhow did the world look like yesterdayâ€, much less â€œ3 months agoâ€. Iâ€™ve seen startups and presidential campaigns and 500 million dollar operations operate like this. ğŸ™ƒ

Now you know the secret. Now you have the magic. What mature companies have been doing all along:Â **snapshot your data daily, append it with datestamps, and write idempotent pipelines on top.**

Thatâ€™s it. Thatâ€™s the whole One Weird Trick. Add aÂ `ds`Â column toÂ _every_Â table. Filter on it. Write your pipelines to be independent of each other. Have every pipeline be ds-aware. Storage is cheap. Your time is expensive. Getting your data wrong isÂ _extra expensive_.

What you get in return:

- Backfills that run in parallel and finish in minutes instead of days
    
- Backfills that are a button push instead of custom SQL mess.
    
- Historical queries that are simpleÂ `WHERE ds=â€™2024-10-01â€™`Â filters instead of date-range gymnastics
    
- Pipelines that are the same whether youâ€™re processing today or reprocessing last year
    
- A built-in time machine for your entire warehouse
    
- Bugs that are fixable instead of permanent scars on your data
    

This is functional data engineering. Functional as in idempotent. And functional as in â€œit worksâ€.

Your backfills are easy now. Your 3am alerts will be rarer. Time complexity is solved. Data recovery is trivial. Your job just becameÂ _so much easier._

But weâ€™re not done yet. Part 3 will tackle: how to scale your team and your warehouse. Parts 4 and 5 are gonna get me back on myÂ [â€œhe who controls metrics controls the galaxyâ€ soapbox](http://integrityinstitute.org/).

For now, go add some datestamps. Your future self will thank you.

### Comments:

https://blog.dataexpert.io/p/stop-using-slowly-changing-dimensions/comment/173661681
Great article! One gap: handling deletes and cancellations

The datestamp approach elegantly handles updates and backfills, but there's a missing piece around data that gets deleted or cancelled after the fact. Here are the main patterns I've seen:

For Dimension Tables:

Soft delete flags in snapshots - Include deleted records in daily snapshots with is_deleted or is_active flags. Queries filter on WHERE current = 1 AND ds = '2025-01-16'. Use a deleted_at timestamp (with microseconds precision) to track exact deletion time. Preserves history while maintaining idempotency.

For Fact Tables:

Reversal/negative records - Like accounting ledgers. Original transaction gets a +100 row, cancellation gets a -100 row with a later ds. Works great for SUM() aggregations but complicates count logic.

Status/flag columns - Each ds includes the status as-of that day (completed, cancelled, refunded). Queries filter WHERE status != 'cancelled'. Better for non-numeric data or complex state transitions.

Separate cancellation fact tables - Keep fct_orders immutable, add fct_order_cancellations. LEFT JOIN and filter. Clean separation of concerns.

---
https://blog.dataexpert.io/p/stop-using-slowly-changing-dimensions/comment/173719193
Date-stamped snapshots are solid for analytic state questions and fast backfills. No argument from me on that, but where this breaks down is human-edited operational data. Salesforce, NetSuite, HubSpot, finance apps. Human entered data... Those records flip after the fact, and auditors expect an immutable trail tied to who changed what and when.

If you need real auditability, capture changes first (CDC or event log). Then layer convenience: a â€œlatestâ€ view for analysts and, if needed, an SCD2 history table for business reporting. Snapshots alone are fine for product analytics; theyâ€™re not enough for regulated ops data.

So the choice isnâ€™t â€œSCD2 vs datestamps.â€ Itâ€™s immutable facts at the base, with snapshots or SCD2 as views that match the model and the risk.

Your question might be but how does SCD2 fix this?

It â€œfailsâ€ only if your ingestion cadence or diff method is too coarse. Fix with CDC or higher-frequency diffs. SCD2 will happily store the extra versions.

Then SCD2 is the storage pattern and CDC is the microscope.