[[#002.4.1.1 Enums]]
[[#002.4.1.2 Little Book of Pipelines Example]]
[[#002.4.1.3 Flexible Schemas]]
[[#002.4.1.4 How is graph data modeling different?]]
## 002.4.1.1 Enums
1. When should you use Enums?
	* Enums are great for low-to-medium cardinality
	* Country is a great example of where enums start to struggle
	* ==Rule of thumb: Keep enums less than 50==
2. Why should you use enums?
	* Built in data quality
	* Built in static fields
	* Built in documentation
3. Enumerations make amazing subpartitions because
	* You have an exhaustive list
	* They chunk up the big data problem into manageable pieces

## 002.4.1.2 Little Book of Pipelines Example

**Use Cases for this Enum Pattern:**
* Whenever you have tons of sources mapped to a shared schema
	* Airbnb:
		* Unit economics (fees, coupons, credits, insurance, infrastructure cost, taxes, etc.)
	* Netflix:
		* Infrastructure Graph (applications, databases, servers, code bases, CI/CD jobs, etc.)
	* Facebook:
		* Family of Apps (oculus, instagram, facebook, messenger, whatsapp, threads, etc.)
[](https://github.com/EcZachly/little-book-of-pipelines#little-book-of-pipelines-example)

Does your pipeline have over 10 unique upstream sources? Do you experience painful backfills? Do you want cleaner lines of responsibility?

Try out the following pattern:

1. Group the sources together into an enumerated group
    
2. Come up with the shared schema. Try to optimize space here since you’re trading off some more storage for easier backfill ability. Partition the table by ds and group.
    
3. Build a Scala enum that keeps track of all the groups and what items are in each group. This enum will store all the DQ-related information and anything else that is a constant value for the given group and item within that group. This enum becomes a form of self-documenting data quality code. There should be an obvious and clean mapping. One Group entry -> One Spark Job.
    
4. Create an abstract class that takes in source function and an entry in the Scala enum.
    
5. Transform the Scala enum into a “little book” Hive table that keeps track of all the groups and items. This table can also be used by data quality and dashboarding

![[2025-09-10 07_15_09-Greenshot.jpg]]

## 002.4.1.3 Flexible Schemas
**How do you model data from disparate sources into a shared schema?**
==Flexible schema!==
1. Benefits
	* You don't have to run ALTER TABLE commands
		* You can just add another key to the map
	* You can manage a lot more columns
	* Your schemas don't have a ton of "NULL" columns
	* "Other_properties" column is pretty awesome for rarely-used-but-needed-columns
2. Drawbacks
	* Compression is usually worse (especially if you use JSON)
		* Maps have the worst compression of all data types
	* Readability, queyability

## 002.4.1.4 How is graph data modeling different?

==Graph modeling is RELATIONSHIP focused, not ENTITY focused==

* Because of this, you can do a very poor job at modeling the entities
	* Usually the model looks like
		* Identifier: STRING
		* Type: STRING
		* Properties: MAP<STRING, STRING>
	* The relationships are modeled a little bit more in depth
		* Usually the model looks like
			* subject_identifier: STRING
			* subject_type: VERTEX_TYPE
			* object_identifier: STRING
			* object_type: VERTEX_TYPE
			* edge_type: EDGE_TYPE
			* properties: MAP<STRING, STRING>
* An edge takes two nodes and connects them together

**Example:**
![[2025-09-10 07_33_46-Greenshot.jpg]]
