* When you add a temporal aspect to your dimensions and the cardinality increases by at least 1 order of magnitude
* Example
	* Airbnb has ~6 million listings
		* If we want to know the nightly pricing and availability of each night for the next year
			* That's 365 * 6 million or about ~ 2 billion nights
		* Should this dataset be:
			* Listing-level with an array of nights?
			* Listing night level with 2 billion rows?
		* If you do the sorting right, Parquet will keep these two about the same size
			* If you have duplicates of the same value in a column (i.e. if modeling at the night level, you will have duplicate dates) they can be smashed up to one.  Then the rest can be nullified and removed from the dataset
### Badness of denormalized temporal dimensions
* If you explode it out and need to join other dimensions
	* A Spark shuffle will ruin your compression because it will mess up the sorting

### Run-length encoding compression
* ==Probably the most important compression technique in big data right now==
	* It's why Parquet file format has become so successful
* If you have a bunch of duplicate data, then it gets nullified
* Shuffle can ruin this.  BE CAREFUL.
	* Shuffle happens in distributed environments when you do JOIN and GROUP BY

* Example:
	* Original data set
	![[2025-09-02 14_02_31-Greenshot.jpg]]

	* Encoded data set
	![[2025-09-02 14_02_54-Greenshot 1.jpg]]

	* Spark shuffle
	![[2025-09-02 14_06_50-Greenshot.jpg]]
	