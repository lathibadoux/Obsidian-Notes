[[#006.3.2.1 Creating vector indexes]]
	[[#006.3.2.1.1 pgvector]]
	[[#006.3.2.1.2 Creating an IVFF index]]
	[[#006.3.2.1.3 IVFF Configuration Tuning ]]
	[[#006.3.2.1.4 Testing IVFF Performance]]
	[[#006.3.2.1.5 Vector index scans with WHERE + ORDER BY]]
[[#006.3.2.2 Adding memory to AI code reviewer]]
	[[#006.3.2.2.1 GitHubPRReviewer]]
	[[#006.3.2.2.2 get_pr_number_from_env]]
	[[#006.3.2.2.3 review_code_with_ai function]]
	[[#006.3.2.2.4 should_review_file]]
	[[#006.3.2.2.5 run_review function]]
	[[#006.3.2.2.6 main function]]
[[#006.3.2.3 load_embeddings.py]]
	[[#006.3.2.3.1 get_embeddings function]]
	[[#006.3.2.3.2 format_job_context function]]
	[[#006.3.2.3.3 load_vectors_into_supabase function]]
	[[#006.3.2.3.4 load_jobs_into_rag function]]
	[[#006.3.2.3.5 load_profiles_into_rag function]]
	[[#006.3.2.3.6 main function]]
## 006.3.2.1 Creating vector indexes

### 006.3.2.1.1 pgvector
### pgvector

* The IVFF and HNSW indexes will be created in Supabase via the Postgres extension `pgvector`.
* Supabase automatically has the extension `pgvector` installed.

`pgvector` adds the following:

1. A new data type:
```sql
vector(n)
```
*(for storing embeddings)*

---

2. Distance operators:
```sql
<-> -- Euclidean distance
<=> -- cosine distance (i.e. cosine similarity)
<#> -- inner product distance
```
* The operators (`<->, <=>, <#>`) mean "Compute the **distance** between two vectors."
* It's used anywhere you want to **measure similarity** - most often in a `SELECT` query's `ORDER BY` clause.

---
 
 **Example to query embeddings table with index help:**
```sql
SELECT id, content
FROM rag_content
ORDER BY embedding <=> :query_embedding
LIMIT 5;
```
* This query says "Find the 5 rows whose embeddings are *closest* to my query vector."
* The `:` sytnax means "Substitute a parameter called `query_embedding` here."
	* In your FastAPI (or Python) code, you'll pass the actual vector as a parameter.

---

**Example to create `query_embedding`:**
 ```python
 query_embedding = openai.embeddings.create(
    model="text-embedding-3-small",
    input="What is IVFFlat indexing?"
)["data"][0]["embedding"]
 ```

---

3. Specialized index types:
```sql
ivfflat
hnsw
```

---

4. Operator classes for similarity metrics:
```sql
vector_l2_ops
vector_ip_ops
vector_cosine_ops
```
* Tells Postgres *how* to calculate that distance under the hood.

---

**The three `pgvector` operator classes:

| Operator class      | Distance metric                 | Typical use case                                   | Operator |
| ------------------- | ------------------------------- | -------------------------------------------------- | -------- |
| `vector_l2_ops`     | **Euclidean distance (L2)**     | Images, embeddings where magnitude matters         | <->      |
| `vector_ip_ops`     | **Inner product (dot product)** | Search models optimized for dot-product similarity | <#>      |
| `vector_cosine_ops` | **Cosine distance**             | Text embeddings (OpenAI, Cohere, etc.)             | <=>      |

---

**Why `pgvector` operator classes matter:**
* You have to match the **operator class** to the **embedding model's training metric**.

|Model|Metric used during training|Recommended operator class|
|---|---|---|
|OpenAI `text-embedding-3-small` / `large`|cosine similarity|`vector_cosine_ops`|
|Cohere|cosine similarity|`vector_cosine_ops`|
|SentenceTransformers (default)|cosine similarity|`vector_cosine_ops`|
|CLIP (image-text)|inner product|`vector_ip_ops`|
|Some custom ML embeddings|Euclidean (L2)|`vector_l2_ops`

---
### 006.3.2.1.2 Creating an IVFF index

* Will smash our vector base into 100 clusters (lists = 100)
* How this is "chunked-up" is based on the vector cosine similarity 
	* cosine similarity = `vector_cosine_ops`

```sql
	BEGIN;
		SET LOCAL maintenance_work_mem = '128MB'; -- WE need more memory to actually create this index!
		CREATE INDEX ivff_index
		  ON rag_content USING ivfflat (embedding vector_cosine_ops)
	   WITH (lists = 100);
	COMMIT;
	```

| Clause                          | Meaning                                                                                                  |
| ------------------------------- | -------------------------------------------------------------------------------------------------------- |
| `USING ivfflat`                 | Use the **IVFFlat algorithm** — partitions vectors into “lists” (clusters) for faster approximate search |
| `(embedding vector_cosine_ops)` | Index the `embedding` column using cosine similarity                                                     |
| `WITH (lists = 100)`            | Create 100 clusters — tuning this affects recall vs speed                                                |

---
### 006.3.2.1.3 IVFF Configuration Tuning

#### At time of index creation
- **If rows ≤ ~1,000,000:** set `lists ≈ rows / 1000`
    (e.g., 80k rows → `lists = 80`; 750k rows → `lists = 750`)
    
- **If rows > ~1,000,000:** set `lists ≈ sqrt(rows)`
    (e.g., 5,000,000 rows → `lists ≈ 2236`)

#### At time of query

Tune how many buckets you want to search with `probes`:
```sql
SET ivfflat.probes = 10;
```

This means: "only search the 10 closest lists to the query vector."
- Higher probes → more accurate, but slower.
- Lower probes → faster, but less accurate.

**Example configuration tuning:**
Let’s say you have 100,000 vectors:

| Setting        | Description                           | Result                             |
| -------------- | ------------------------------------- | ---------------------------------- |
| `lists = 100`  | 100 clusters (≈1000 vectors per list) | Balanced                           |
| `probes = 10`  | Search 10 lists                       | ~10% of data scanned               |
| `probes = 100` | Search all lists                      | Exact search (same as brute-force) |

---

### 006.3.2.1.4 Testing IVFF Performance

**Query with index:**
```sql
EXPLAIN (ANALYZE, BUFFERS)
SELECT id
FROM rag_content
ORDER BY embedding <=> (SELECT embedding FROM rag_content LIMIT 1) -- or <-> / <#> depending on your metric
LIMIT 10;
```
* *The code block `SELECT embedding FROM rag_content LIMIT 1` selects one embedding from the embedding table at random to compare every stored embedding in the table against.

**Output with index:**

|                                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------------------------------- |
| QUERY PLAN                                                                                                                             |
| Limit  (cost=12.26..24.82 rows=10 width=55) (actual time=2.810..2.932 rows=8 loops=1)                                                  |
| Buffers: shared hit=200                                                                                                                |
| InitPlan 1                                                                                                                             |
| ->  Limit  (cost=0.00..0.10 rows=1 width=18) (actual time=0.019..0.020 rows=1 loops=1)                                                 |
| Buffers: shared hit=2                                                                                                                  |
| ->  Seq Scan on rag_content rag_content_1  (cost=0.00..102.50 rows=1050 width=18) (actual time=0.019..0.019 rows=1 loops=1)            |
| Buffers: shared hit=2                                                                                                                  |
| ->  ==Index Scan using ivff_index on rag_content  (cost=12.16..1330.75 rows=1050 width=55) (actual time=2.809..2.928 rows=8 loops=1)== |
| Order By: (embedding <=> (InitPlan 1).col1)                                                                                            |
| Buffers: shared hit=200                                                                                                                |
| Planning:                                                                                                                              |
| Buffers: shared hit=158                                                                                                                |
| Planning Time: 1.654 ms                                                                                                                |
| Execution Time: 3.075 ms                                                                                                               |

---

**Query without index:**
```sql
EXPLAIN (ANALYZE, BUFFERS)
SELECT id
FROM rag_content
```

**Output without index:**

|                                                                                                                             |
| --------------------------------------------------------------------------------------------------------------------------- |
| QUERY PLAN                                                                                                                  |
| Limit  (cost=127.91..127.94 rows=10 width=55) (actual time=10.411..10.415 rows=10 loops=1)                                  |
| Buffers: shared hit=6420                                                                                                    |
| InitPlan 1                                                                                                                  |
| ->  Limit  (cost=0.00..0.10 rows=1 width=18) (actual time=0.008..0.008 rows=1 loops=1)                                      |
| Buffers: shared hit=2                                                                                                       |
| ->  Seq Scan on rag_content rag_content_1  (cost=0.00..102.50 rows=1050 width=18) (actual time=0.007..0.007 rows=1 loops=1) |
| Buffers: shared hit=2                                                                                                       |
| ->  Sort  (cost=127.82..130.44 rows=1050 width=55) (actual time=10.410..10.411 rows=10 loops=1)                             |
| Sort Key: ((rag_content.embedding <-> (InitPlan 1).col1))                                                                   |
| Sort Method: top-N heapsort  Memory: 25kB                                                                                   |
| Buffers: shared hit=6420                                                                                                    |
| ->  Seq Scan on rag_content  (cost=0.00..105.12 rows=1050 width=55) (actual time=0.100..10.056 rows=1050 loops=1)           |
| Buffers: shared hit=6417                                                                                                    |
| Planning:                                                                                                                   |
| Buffers: shared hit=154                                                                                                     |
| Planning Time: 0.907 ms                                                                                                     |
| Execution Time: 10.487 ms                                                                                                   |

---
### 006.3.2.1.5 Vector index scans with WHERE + ORDER BY

 You cannot construct the b-tree and IVFF (or HSNW) separately and need to be done at the same time in order for `pg_vector` and Postgres query optimizer to pick the most efficient path.

 The solution is "partial" vector indexes that act as a filtered-down version of the graph.

 That way you create partial indices in Postgres is by specifying the `WHERE` clause when you're creating the index.

```sql
CREATE INDEX jobs_ivff_index ON rag_content USING ivfflat (embedding vector_cosine_ops) WHERE document_type = 'job';
```

```sql
CREATE INDEX profiles_ivff_index ON rag_content USING ivfflat (embedding vector_cosine_ops) WHERE document_type = 'profile';
```


---
## 006.3.2.2 Adding memory to AI code reviewer

* Goal: Want to give the ai_code_reviewer.py the ability to reference previous pull requests so that it can build a history.  We want to build memory into the code reviewer.



**ai_code_reviewer.py**
```python
#!/usr/bin/env python3

"""

AI Code Reviewer - Automated code review using OpenAI GPT models

Analyzes PR changes and posts review comments directly to GitHub

"""

  

import os

import sys

import json

from typing import List, Optional

from github import Github

from openai import OpenAI

from load_embeddings import load_vectors_into_supabase, get_embedding

from supabase_lib import query_rag_content

```

#### 006.3.2.2.1 GitHubPRReviewer
This class initializer:
1. Connects to GitHub and OpenAI APIs
2. Figures out which repository and PR to look at
3. Builds a unique document ID for later database use
4. Validates input
5. Retrieves the live GitHub PR object

```python

class GitHubPRReviewer:

# set-up or "initialization" phase
    def __init__( 

        self,

        github_token: str, # lets it authenticate to GitHub’s API

        openai_api_key: str, # lets it talk to OpenAI’s API (for code review, summarization, etc.)

        model: str = "gpt-4o-mini", #specifies which OpenAI model to use (defaults to `"gpt-4o-mini"`)

        repository: Optional[str] = None, # optional, name of the GitHub repo (like `"username/repo-name"`)

        pr_number: Optional[int] = None, # optional, the PR number to review

    ):

# Step 1: Create GitHub and OpenAI clients.  These lines create connections to those services
        self.github_client = Github(github_token) # from the **PyGithub** library, which allows interacting with GitHub’s REST API

        self.openai_client = OpenAI(api_key=openai_api_key) # the official OpenAI Python client


# Step 2: Store basic i no
        self.model = model

        self.repository_name = repository or os.getenv("GITHUB_REPOSITORY") # If `repository` was provided, use it. If not, try to read it from an **environment variable** (`GITHUB_REPOSITORY`), which GitHub Actions automatically sets (e.g. `"lathibadoux/ai-application-october-2025"`).

        self.pr_number = pr_number or self._get_pr_number_from_env() # if not passed in, try to detect it from the environment via a helper function `_get_pr_number_from_env()`


# Step 3: Define a unique document ID
        self.document_id = self.repository_name + '/pulls/' + str(self.pr_number)
# document_id is used later on in load_vectors_into_supabase
# This ID can be used later to store or retrieve embeddings from a vector database (e.g. Supabase), to keep track of which PR the data belongs to
  

# Step 4: Sanity checks
        if not self.repository_name:

            raise ValueError("Repository not specified and GITHUB_REPOSITORY env var not set")

        if not self.pr_number:

            raise ValueError("PR number not specified and could not be determined from environment")

  
# Retrieve actual GitHub objects

        self.repo = self.github_client.get_repo(self.repository_name) # retrieves the repository object

        self.pull_request = self.repo.get_pull(self.pr_number) # retrieves the pull request object (which contains all the PR’s metadata, files, comments, commits, etc.)

 ``` 

#### 006.3.2.2.2 get_pr_number_from_env
```python

    def _get_pr_number_from_env(self) -> Optional[int]:

        """Extract PR number from GitHub Actions environment variables"""

        # Try getting from GITHUB_REF (format: refs/pull/123/merge)

        github_ref = os.getenv("GITHUB_REF", "")

        if github_ref.startswith("refs/pull/"):

            try:

                pr_num = int(github_ref.split("/")[2])

                return pr_num

            except (IndexError, ValueError):

                pass

        return None
```

#### 006.3.2.2.3 review_code_with_ai function
For a single changed file, it:
1. Builds a unique ID for this PR+file
2. Searches your vector DB for past reviews of the _same file_ (RAG context)
3. Builds a prompt (includes historical diffs + current diff)
4. Calls the OpenAI chat completions API
5. Stores the new review (diff + AI response) back into the vector DB
6. Returns the AI’s review text
```python
# Method to review one file's changes
def review_code_with_ai(self, filename: str, diff: str, file_content: Optional[str] = None) -> Optional[str]:
# Inputs: the `filename`, the unified `diff` text, and an optional `file_content` (not used here)
# Returns: `str` review text or `None` on error

# Step 1: Builds a unique row key for this PR+file review chunk
# Example: `owner/repo/pulls/42-path/to/file.py` 
        id = self.document_id + '-' + filename 

# Step 2: Create embedding vector for semantic search using the concatenated filename + diff.
# Will be used to query similar past chunks.
        filename_embedding = get_embedding(filename + diff)

# Step 3: Queries vector database (RAG) for up to 10 semantically similar chunks of type pr_chunk.
# Returns: historical context about this file’s prior changes/reviews from Supabase
        memory_context = query_rag_content(filename_embedding, 10, 'pr_chunk') 

        previous_changes = [] # prepares to collect the diff parts from previous similar chunks


        if memory_context.data: # if the vector query returns results

            for data in memory_context.data: # loop each data row

                memory_filename = data['id'].split('-')[-1] # extract the filename from the row’s `id` (everything after the last hyphen)

                if memory_filename == filename and data['id'] != id: # only consider rows for the **same filename** and **not** this exact row (`data['id'] != id`)

                    context = data['context'] # from the stored context blob (which contains “Diff: … AI Response: …”) slice the substring between the markers: from `'Diff'` up to `'AI Response'`

                    diff_context = context[context.index('Diff'):context.index('AI Response')]

                    previous_changes.append(diff_context) # append that extracted diff text to `previous_changes`

  

        previous_changes_str = '<PREVIOUS_CHANGE>'.join(previous_changes) # join all prior diff contexts with a delimiter `<PREVIOUS_CHANGE>`

        previous_changes_prompt = f"""Historical Changes: {previous_changes_str or 'None'}""" # build a short “Historical Changes:” prompt segment. If none found, use `None`

# Step 4: Prompts AI
        prompt = f"""You are an expert code reviewer. Review the following code changes and provide constructive feedback.

{previous_changes_prompt}

File: {filename}

Diff:


{diff}

  

Please provide:

1. Any bugs or potential issues

2. Code quality improvements

3. Security concerns

4. Best practice suggestions

5. A history of the changes to this file if they are provided

  

Be concise and actionable. If the code looks good, say so briefly and do not mention the small issues.

  

At the end, make sure to grade the pull request and suggest whether it is ready to merge

  

"""

        print(prompt) # logs the exact prompt (useful for debugging; be mindful of sensitive code in logs)

# Calls the OpenAI Chat Completions API
        try:

            response = self.openai_client.chat.completions.create(

                model=self.model, # `model`: whatever was set earlier (default `"gpt-4o-mini"`)

                messages=[

                    {"role": "system", "content": "You are an expert code reviewer providing constructive feedback on code changes."},

                    {"role": "user", "content": prompt}

                ], # messages: system role (sets behavior) + user prompt (the content above).

                temperature=0,

                max_tokens=1000,

            )

  
# AI response
# Extract the model’s text output from the first choice and trim whitespace
# Overwrites the `response` variable with the string content
            response = response.choices[0].message.content.strip()

  
# Step 5: Store returned prompt into vector database to retrieve for future embeddings

  # Build a new “context blob” that captures:
  # filename
  # the exact diff reviewed
  # the AI's response
            context = f""" 
            
            File Name:{filename} 

            Diff: {diff}

            AI Response: {response}

            """

# get_embedding imported from load_embeddings.py
            embedding = get_embedding(context) # create an embedding for that full context blob

  
# From load_embeddings.py file.  Persist the review back into your vector DB
            load_vectors_into_supabase(id, embedding, context, 1, 'pr_chunk',

                                       document_id=self.document_id,

                                       username=self.pull_request.user.url

                                       )

            return response # return the AI’s review text to the caller (so the caller can assemble the PR comment).

# If anything fails (API error, indexing error, etc.), log to stderr and signal failure with `None`
        except Exception as e:

            print(f"Error calling OpenAI API: {e}", file=sys.stderr)

            return None
```
```
```

#### 006.3.2.2.4 should_review_file
```python

    def should_review_file(self, filename: str, exclude_patterns: List[str]) -> bool:

        """Check if file should be reviewed based on exclude patterns"""

        for pattern in exclude_patterns:

            # Simple glob pattern matching

            if pattern.startswith("**/"):

                # Match anywhere in path

                if filename.endswith(pattern[3:]) or pattern[3:] in filename:

                    return False

            elif pattern.startswith("*."):

                # Match extension

                if filename.endswith(pattern[1:]):

                    return False

            elif pattern in filename:

                return False

        return True
```

#### 006.3.2.2.5 run_review function

| Step | Action                            | Example                       |
| ---- | --------------------------------- | ----------------------------- |
| 1    | Connects to PR and starts logging | “Starting AI code review…”    |
| 2    | Fetches changed files             | 5 files changed               |
| 3    | Filters/excludes/deleted files    | Skips test files, binaries    |
| 4    | Sends code diffs to OpenAI        | `review_code_with_ai()`       |
| 5    | Gathers all AI responses          | 3 reviews generated           |
| 6    | Posts summary comment on PR       | Comment: “## AI Code Review…” |

```python

    def run_review(self, exclude_patterns: Optional[List[str]] = None) -> None: # `exclude_patterns`: lets you tell the reviewer to skip certain files (e.g. `["*.json", "tests/*"]`)
# Returns `None`, because all results are printed or posted directly to GitHub
        """Main review process"""

        exclude_patterns = exclude_patterns or [] # If no patterns are passed, it defaults to an empty list.


        print(f"Starting AI code review for PR #{self.pr_number} in {self.repository_name}") # logs which PR it’s about to review

  

# Step 1: This uses the GitHub API (via PyGithub) to get all files that were modified in the PR.
# Each `file_obj` includes metadata like:
# `filename`
# `status` (“added”, “modified”, or “removed”)
# `patch` (the unified diff text showing changes)
        files = list(self.pull_request.get_files())


        print(f"Found {len(files)} changed files")

# Step 2: Prepare containers
        reviewed_files = [] # Which files were actually reviewed

        review_summary = [] # The text of each file’s AI-generated review (to be posted later)


# Step 3: Loop through each changed file in the PR
        for file_obj in files:

            filename = file_obj.filename

  
# Step 4: Skip files based on patters
# This uses another helper method (`should_review_file`) to decide whether to skip files that match any exclusion pattern (e.g., skip `README.md` or `/tests/`)
            if not self.should_review_file(filename, exclude_patterns):

                print(f"Skipping {filename} (excluded)")

                continue

  
# Step 5: Skip deleted files

            if file_obj.status == "removed":

                print(f"Skipping {filename} (deleted)")

                continue

# Step 6: Get the code diff.
#The `patch` attribute contains the _unified diff_ — basically, lines of code added/removed.  
#If there’s no diff (sometimes happens with binary files or renames), it skips.
            diff = file_obj.patch or ''

            if not diff:

                print(f"Skipping {filename} (no diff)")

                continue

# Step 7: Send the diff to OpenAI.
            print(f"Reviewing {filename}...")
# Calls another method `review_code_with_ai()`, which:
# Sends a prompt like “Review this code diff for potential issues or improvements.”
# Uses the model defined in `self.model` (e.g. `gpt-4o-mini`)
# Returns the AI’s textual feedback
            review = self.review_code_with_ai(filename, diff)

# Step 8: Store results (if AI produced a response)
            if review:

                reviewed_files.append(filename) # store filename for tracking

                review_summary.append(f"### {filename}\n\n{review}") # store formatted markdown review text (to combine later)


# Step 9: Once all files are processed, post a summary comment on GitHub
        if review_summary:

# builds a markdown-formatted comment combining all file reviews
            summary = "## AI Code Review\n\n" + "\n\n---\n\n".join(review_summary)

            summary += f"\n\n---\n*Reviewed by {self.model}*"

            self.pull_request.create_issue_comment(summary) # posts that comment **directly to the PR thread** on GitHub via `create_issue_comment()`

            print(f"Review posted successfully for {len(reviewed_files)} files") # logs how many files were reviewed

        else:

            print("No files to review")

```


#### 006.3.2.2.6 main function
* Gets the token and API key and creates a GitHub reviewer object and runs the review
* ```python   

def main():

    """Main entry point"""

    # Get configuration from environment variables

    github_token = os.getenv("GITHUB_TOKEN")

    openai_api_key = os.getenv("OPENAI_API_KEY")

    model = os.getenv("OPENAI_API_MODEL", "gpt-4o-mini")

    exclude = os.getenv("EXCLUDE", "")

  

    if not github_token:

        print("Error: GITHUB_TOKEN environment variable not set", file=sys.stderr)

        sys.exit(1)

  

    if not openai_api_key:

        print("Error: OPENAI_API_KEY environment variable not set", file=sys.stderr)

        sys.exit(1)

  

    # Parse exclude patterns

    exclude_patterns = [p.strip() for p in exclude.split(",") if p.strip()]

  

    try:

        reviewer = GitHubPRReviewer(

            github_token=github_token,

            openai_api_key=openai_api_key,

            model=model,

        )

        reviewer.run_review(exclude_patterns=exclude_patterns)

        print("Code review completed successfully")

    except Exception as e:

        print(f"Error during code review: {e}", file=sys.stderr)

        sys.exit(1)

  
  

if __name__ == "__main__":

    main()
```

---
### 006.3.2.3 load_embeddings.py
**High-level flow:**
1. Read JSON files (jobs, profiles).
2. Convert each record to a formatted text **context** string.
3. Call OpenAI embeddings on that string.
4. Upsert one row per record into `rag_content` via Supabase.

**What each layer is responsible for:**

| Function                       | Purpose                                      | Hits `rag_content`? | How many times                      |
| ------------------------------ | -------------------------------------------- | ------------------- | ----------------------------------- |
| `get_embedding()`              | Calls OpenAI API and returns a vector        | ❌                   | 0                                   |
| `load_vectors_into_supabase()` | Inserts a row into `rag_content`             | ✅                   | Once per record                     |
| `load_jobs_into_rag()`         | Reads job JSON and loops through entries     | ⚙️ indirectly       | N times (via calls to load_vectors) |
| `load_profiles_into_rag()`     | Reads profile JSON and loops through entries | ⚙️ indirectly       | M times (via calls to load_vectors) |
| `main()`                       | Orchestrates the whole process               | ❌                   | 0                                   |


```python
#!/usr/bin/env python3

"""

Script to read JSON files from the data folder, convert them to embeddings,

and load them into the Supabase rag_content table.

"""

  

import json

import os

from pathlib import Path

from dotenv import load_dotenv

from supabase import create_client, Client

from openai import OpenAI

  

# Load environment variables

load_dotenv()

  

# Initialize clients

supabase: Client = create_client(

    os.getenv("SUPABASE_URL"),

    os.getenv("SUPABASE_KEY")

)

openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
```

#### 006.3.2.3.1 get_embeddings function

**Purpose:** Wraps the OpenAI Embeddings API call.
- **Inputs:** `text` (string to embed), optional model name.
- **Output:** Python list of floats = embedding vector. (type hint `-> list[float]`)
- **Side effects:** None.
- **Hidden dependency:** `openai_client` must be defined/initialized elsewhere (not shown here). If it isn’t, this will crash.

```python
def get_embedding(text: str, model: str = "text-embedding-3-small") -> list[float]:

    """

    Generate an embedding for the given text using OpenAI's API.

  

    Args:

        text: The text to embed

        model: The embedding model to use (default: text-embedding-3-small)

  

    Returns:

        A list of floats representing the embedding vector

    """

    response = openai_client.embeddings.create(

        input=text,

        model=model

    )

    return response.data[0].embedding
```

#### 006.3.2.3.2 format_job_context function

**Purpose:** Converts a job JSON object into a **single string** that’s friendly to embed.
- **Inputs:** `job` dict.
- **Output:** Multi-line string containing job fields (title, company, skills, description, etc.).
- **Side effects:** None.

```python  

def format_job_context(job: dict) -> str:

    """

    Format a job posting into a text string for embedding.

  

    Args:

        job: Dictionary containing job information

  

    Returns:

        Formatted string representation of the job

    """

    skills_str = ", ".join(job.get("skills", []))

    return f"""Job Title: {job.get('title', 'N/A')}

Company: {job.get('company', 'N/A')}

Location: {job.get('location', 'N/A')}

Employment Type: {job.get('employment_type', 'N/A')}

Experience Level: {job.get('experience_level', 'N/A')}

Salary Range: {job.get('salary_range', 'N/A')}

Skills: {skills_str}

Description: {job.get('description', 'N/A')}"""

  
  

def format_profile_context(profile: dict) -> str:

    """

    Format a user profile into a text string for embedding.

  

    Args:

        profile: Dictionary containing profile information

  

    Returns:

        Formatted string representation of the profile

    """

    skills_str = ", ".join(profile.get("skills", []))

    education_str = "; ".join([

        f"{edu.get('degree', '')} from {edu.get('school', '')}"

        for edu in profile.get("education", [])

    ])

  

    return f"""Name: {profile.get('name', 'N/A')}

Title: {profile.get('title', 'N/A')}

Company: {profile.get('company', 'N/A')}

Location: {profile.get('location', 'N/A')}

Experience: {profile.get('experience_years', 0)} years

Career Level: {profile.get('career_level', 'N/A')}

Industry: {profile.get('industry', 'N/A')}

Skills: {skills_str}

Education: {education_str}

Summary: {profile.get('summary', 'N/A')}

LinkedIn: {profile.get('linkedin_url', 'N/A')}"""
```

#### 006.3.2.3.3 load_vectors_into_supabase function
==**The only function that actually writes data to Supabase**==

**Purpose:** Centralized “write a row” function.
- **Inputs:**
    - `id`: primary key you’re using for `rag_content` row (string).
    - `embedding`: the vector (`list[float]`).
    - `context`: the raw text you embedded (stored as `context` column).
    - `user_id`: owner (int).
    - `document_type`: categorization (e.g., `"job"`, `"profile"`).
    - `**kwargs`: any additional columns to include (this is where your Week 2 **metadata** fields go).
- **Output:** None; prints success/failure.
- **Hidden dependency:** `supabase` client must be initialized elsewhere.
- **Upsert behavior:** Will insert or update on conflict using Supabase defaults.
- **Notes for Week 2:** This is your hook.
    - Keep `context` as the paragraph text.
    - Pass metadata via `**kwargs` (e.g., `document_id`, `chapter_number`, `chapter_title`, `paragraph_number`, `language`, etc.).
    - Consider using a deterministic `id` such as `f"{document_id}:{chapter_number}:{paragraph_number}"` to avoid duplicates.

```python
def load_vectors_into_supabase(id, embedding, context, user_id, document_type, **kwargs):

    # Insert into Supabase

        data = {

            "id": id,

            "embedding": embedding,

            "context": context,

            "user_id": user_id,

            "document_type": document_type,

            **kwargs

        }

  

        try:

            supabase.table("rag_content").upsert(data).execute()

            print(f"  ✓ Inserted job {id}")

        except Exception as e:

            print(f"  ✗ Error inserting vectors {id}: {e}")
```

#### 006.3.2.3.4 load_jobs_into_rag function
* Reads a local JSON file, formats each job, and calls `get_embeddings()` and `load_vectors_into_supabase()` for each record.

**Chain of calls:**
```python
main()
  ├── load_jobs_into_rag()
  │     ├── get_embedding()             # calls OpenAI API
  │     └── load_vectors_into_supabase()  # ✅ writes to rag_content
  └── load_profiles_into_rag()
        ├── get_embedding()
        └── load_vectors_into_supabase()  # ✅ writes to rag_content

```

**Purpose:** Pipeline to read **jobs** JSON and write to DB.
- **Flow:**
    1. `json.load()` the file.
    2. For each job: make `job_id`, format context, embed, upsert into `rag_content` with `document_type="job"`.

```python
def load_jobs_into_rag(file_path: str, user_id: int = 1):

    """

    Load job postings from a JSON file into the rag_content table.

  

    Args:

        file_path: Path to the JSON file containing job postings

        user_id: User ID to associate with the content (default: 1)

    """

    print(f"Loading jobs from {file_path}...")

  

    with open(file_path, 'r') as f:

        jobs = json.load(f)

  

    for i, job in enumerate(jobs):

        job_id = f"job_{job.get('id', i)}"

        context = format_job_context(job)

  

        print(f"Processing job {i+1}/{len(jobs)}: {job.get('title', 'Unknown')}...")

        # Generate embedding

        embedding = get_embedding(context)

        load_vectors_into_supabase(job_id, embedding, context, user_id, "job")

  

    print(f"Completed loading {len(jobs)} jobs.\n")
```

#### 006.3.2.3.5 load_profiles_into_rag function
```python
def load_profiles_into_rag(file_path: str, user_id: int = 1):

    """

    Load user profiles from a JSON file into the rag_content table.

  

    Args:

        file_path: Path to the JSON file containing profiles

        user_id: User ID to associate with the content (default: 1)

    """

    print(f"Loading profiles from {file_path}...")

  

    with open(file_path, 'r') as f:

        profiles = json.load(f)

  

    for i, profile in enumerate(profiles):

        profile_id = f"{profile.get('linkedin_url', i)}"

        context = format_profile_context(profile)

  

        print(f"Processing profile {i+1}/{len(profiles)}: {profile.get('name', 'Unknown')}...")

        # Generate embedding

        embedding = get_embedding(context)

        load_vectors_into_supabase(profile_id, embedding, context, user_id, "profile")

    print(f"Completed loading {len(profiles)} profiles.\n")
```

#### 006.3.2.3.6 main function

**Purpose:** Entry point that loads **JSON** from `data/` and pushes jobs + profiles.
- **Flow:**
    - Computes `data_dir = Path(__file__).parent / "data"`.
    - If `synthetic_data_jobs.json` exists → call `load_jobs_into_rag`.
    - If `synthetic_profiles.json` exists → call `load_profiles_into_rag`.

```python
def main():

    """

    Main function to load all JSON files from the data folder.

    """

    data_dir = Path(__file__).parent / "data" # Builds an absolute path to a data/ folder **next to this .py file**, regardless of where you run the script from.
# Path(__file__).parent → the folder containing the script.
# This prevents “can’t find file” errors caused by the current working directory.

# The print("=" * 60) lines are just visual separators in the terminal so you can see the run phases clearly.
    print("=" * 60) 

    print("Starting RAG Content Loading Process")

    print("=" * 60)

    print()

  

    # Load jobs
    
    # jobs_file = data_dir / "synthetic_data_jobs.json"
    # profiles_file = data_dir / "synthetic_profiles.json"
    # These create full paths like /.../your_repo/.../data/synthetic_data_jobs.json
    jobs_file = data_dir / "synthetic_data_jobs.json"

    if jobs_file.exists(): # Guarded execution with .exists()
    # If the file exists → call the corresponding loader.
    # If not → print a warning and skip.  
    # This makes the script **idempotent and flexible**: it runs even if one of the inputs is missing.

        load_jobs_into_rag(str(jobs_file)) # Call the specific loaders
# - open the file, loop records, format text, embed, call load_vectors_into_supabase(...) to upsert into rag_content.

    else:

        print(f"Warning: {jobs_file} not found, skipping jobs.\n")

  

    # Load profiles

    profiles_file = data_dir / "synthetic_profiles.json"

    if profiles_file.exists(): 

        load_profiles_into_rag(str(profiles_file))

    else:

        print(f"Warning: {profiles_file} not found, skipping profiles.\n")

# Footer prints
# Signals completion so you can see a clean end in the logs.
    print("=" * 60)

    print("RAG Content Loading Complete!")

    print("=" * 60)

  
  

if __name__ == "__main__": # Standard Python entry-point pattern.

    main() # Means:only run main() when you execute this file directly (python load_embeddings.py), not when it’s imported from another file.
    # This lets you reuse functions from elsewhere without triggering the whole load process.
```