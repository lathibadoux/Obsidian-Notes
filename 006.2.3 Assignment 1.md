### Prompting - file you need to submit is prompts.md
- Prompt engineer the following question so it consistently answers the question right the first time. What country has the same letter repeated the most in its name?
- Try different models, which one performs better?
- Try using GPT-5 and prompt engineering to get a more accurate answer.
- In your prompts.md file include:
- The best performing models you tried
- The prompt engineered version to get GPT-5 to work better
- The actual answer to the question
    
### Vibe Coding - file you need to submit is main.py
- Follow the exercises in lab 2 where we resume parser
- Extend the `/api/parse-resume` end point to accept images and PDFs to make the AI truly multi-modal.
- Note - OpenAI handles PDFs through the Files API and images through base64 encoding.

**prompts.md**
- Present: Mentions of higher-performing models. You cited multiple advanced models you tried; to strengthen this, consider naming concrete, widely recognized model versions as well. 
- Present: Clear example prompts demonstrating prompt-engineering techniques:
- Clear task statement and explicit Instructions list with constraints on data source (UN M49 English list). 
- Requirement to use full official country names. 
- Step-by-step decomposition and tie-handling criteria. 
- Worked examples to illustrate expectations and formatting. These are solid prompt-engineering elements. 
- Present: The file includes one of the required country names explicitly, as required by the rubric. 

- **Suggestions:**
	- Consider adding guardrails such as output schema constraints and an explicit “no chain-of-thought” direction if you want strictly structured outputs.
	- Add a quick rationale for model selection to justify why those models performed better for this task. 

**main.py**
- Present: FastAPI endpoint /api/parse-resume exists and correctly accepts a file via multipart/form-data (UploadFile), with an alternate html_content form field.
- PDF handling: You upload the PDF to the OpenAI Files API, retrieve a file_id, and then pass that along to the completions endpoint as required by the rubric. You also clean up the uploaded file afterward. This meets the rubric.
- Image handling: You base64-encode the image and pass it via data URL in messages content, satisfying the rubric. 
- **Additional strengths:** 
	- Good use of a tools schema for structured output. 
	- Sensible fallbacks (using message.content if no tool_call is produced). 
	- Inserts parsed JSON into Supabase and returns useful response content. - Implementation cautions and improvement suggestions: 
	- API compatibility: Passing a file_id directly to chat completions as a file content block may not be supported across all SDK versions. If you run into issues, consider either: 
	- Using the Assistants API end-to-end for file-based parsing, or 
	- Using the Responses API that supports multimodal inputs with file references. 
	- MIME detection for images: You hardcode image/jpeg in the data URL. Derive the correct MIME type from the uploaded file to avoid misclassification (e.g., png/webp). 
* **Error handling and cleanup:&=** 
	* Ensure file deletion happens in a finally block so cleanup is guaranteed.
	* Add validation around parsed_resume to confirm it’s valid JSON before insert; you already do json.loads, which is good—consider catching JSONDecodeError distinctly. 
* **Security and robustness:** 
	* Add limits on file size and acceptable content types. 
	* Consider basic sanitization and logging hygiene for PII. 
- **DX additions:** 
	- Document expected multipart field names and provide a sample curl request for PDF and image uploads. 
	- Surface clearer error messages when no tool_calls are returned. 


