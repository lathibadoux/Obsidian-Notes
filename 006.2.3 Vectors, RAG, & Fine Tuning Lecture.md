[[#006.2.3.1 Vector Databases]]
[[#006.2.3.2 Retrieval-Augmented Generation (RAG)]]
[[#006.2.3.3 Fine Tuning]]
## 006.2.3.1 Vector Databases
	Vectors give us semantics and statistical meaning!
* Think of a vector as a one-dimensional matrix (one column of numbers).  It's just a list of numbers (either decimal or integer).
* Vectors store unstructured data as vector embeddings (list of numbers)
* A **vector database** stores vectors, text, and metadata. 

**2-Dimensional Vector**
*![[Semantic_Feature_Space.jpg]]

* Graphs the co-occurrences of words.
* Vectors are the co-occurrence weighting of words in English.

---

**Top Open Source Vector Databases (as of 2025)**

| Name                | Language              | Key Strengths                                                                   | License                  |
| ------------------- | --------------------- | ------------------------------------------------------------------------------- | ------------------------ |
| **Chroma**          | Python                | Very easy to use, integrates with LangChain/LlamaIndex, great for local dev     | Apache 2.0               |
| **FAISS** (by Meta) | C++ / Python bindings | Extremely fast in-memory search, supports GPU acceleration                      | MIT                      |
| **Qdrant**          | Rust                  | High performance, rich metadata filters, easy Docker deploy                     | Apache 2.0               |
| **Milvus**          | Go / C++              | Large-scale, production-grade distributed system                                | Apache 2.0               |
| **Weaviate**        | Go                    | Semantic + symbolic hybrid search, GraphQL/REST API                             | BSD-3                    |
| **pgvector**        | Postgres extension    | Turns Postgres into a vector DB; great for structured + semantic hybrid queries | Open Source (PostgreSQL) |

**Hosted / Managed Vector Databases (as of 2025)**

|Service|Backend|Key Features|Notes|
|---|---|---|---|
|**Pinecone**|Proprietary|Fully managed cloud service, highly reliable, pay-per-index|Most popular managed option|
|**Redis Vector Similarity Search**|C / Redis Stack|Combines traditional key-value + vector search|Open-core|
|**ElasticSearch / OpenSearch**|Java|Adds dense vector fields for hybrid keyword + semantic search|Good for enterprises already using Elastic|
|**Vespa.ai**|Java / C++|Search engine + recommender system in one|Used at Yahoo-scale|
|**Neon + pgvector**|Postgres Cloud|Serverless Postgres with vector support|Newer option for hybrid analytics|

---
### What are Emeddings?
An *embedding* is a specific kind of vector that encodes *semantic meaning* - it's created by a machine learning model that "maps" text (or images, etc.) into a vector space where **similar things are close together.**

Once the embeddings are created, we **store them as vectors** - i.e. in `pgvector` the live in a column of type `vector(1536).`
### Dense Vector Embeddings
* Easiest way is to use other LLMS!
* Be careful because the *vector length* is a critical aspect of these embeddings and is fixed once you setup your vector database index!
* EVERY word is semantically connected to another word.  

### Sparse Vector Embeddings?
* Sparse vectors are for exact keyword matching!
* They capture exact matches meaning instead of semantic matches
* EVERY word is NOT semantically connected to another word
* Common when you have rare languages:
	* Legal languages
	* Medical languages
	* Technical languages

* The big difference between dense vectors and sparse vectors is the data type:
	* Sparse vectors are integers
	* Dense vectors are decimals

---
### Embedding Models
* Embedding models convert text (i.e. your question, task, etc) to a list of floating-point numbers (i.e. vector).  
* Each number represents some aspect of the text's *semantic meaning*.

**Example:**
```python
text = "Parental leave is 12 weeks paid."
embedding = [0.172, -0.339, 0.501, ...]  # typically 1,536 dimensions
```

**Embedding Models from OpenAI**
![[Embedding_model_use_cases.jpg]]

* The more dimensions you consider, the higher accuracy the match.  However, it comes at a higher cost.
* Vectors are a fixed size!
	* Remember to not mix and match different embedding models into the same index (IT WILL NOT WORK!)
	* The bigger the vector length, the more accurately it will capture nuances.  AT THE COST OF MEMORY, COMPUTER, and LATENCY OF COURSE!
* Why 1,536 and 3,072?
	* These numbers are still "round"
		* 1536 = 12 * 128
		* 3072 = 24 * 128
	* The reason why they do 12 and 24 instead of 16 and 32 is because ==OpenAI picks the SMALLEST size to attain a certain level of accuracy==

	![[2025-10-24 12_55_38-main.py - ai-engineering-bootcamp-app - Visual Studio Code.jpg]]

* **Attention Heads**
	* Little helper that looks at the words and decides *which other words* are important.
	* Example: "The cat sat on the mat."
		* One attention head might focus on how "cat" connects to "sat", while another might look at "mat".
	* Each head produces **128 numbers** that represent what it noticed.
* **The Layer**
	* Like a control center that gathers all the information from those 12 heads.
	* Mixes, filters, and refines the signals they found - sort of like combining opinions from 12 experts into one detailed answer.
* **Output Embedding (1,536 dimensions)**
	* After all that processing, the model creates a **dense vector** - a list of **1,536 numbers**.
	* That's the model's *understanding* of the word, sentence, or token at that moment.
	* Each of those numbers holds a tiny piece of meaning (like tone, grammar, emotion, topic, etc.)

### Vectors search optimize for one metric!
* There are essentially two choices here:
	* **Cosine Similarity**:
		* Better suited for text
		* Focuses on angular similarity (are the vectors pointed in the same way)
		* 90% of projects will use cosine distance
		
		![[Cosine_distance.jpg]]

	* **Euclidean Distance:**
		* better suited for images and video
		* Focuses on absolute distance (how far apart are these vectors)

		![[euclidean_distance.jpg]]


## 006.2.3.2 Retrieval-Augmented Generation (RAG)

### Core Idea
==In extremely simple terms, RAG fetches additional data to make your AI hallucinate less!==

Rag combines two main components:
1. **Retrieval** - fetching relevant information from a data source (like documents, databases, or the web).
2. **Generation** - using that retrieved information to produce a coherent, context-aware answer with a language mode (like GPT).

What RAG does:
* RAG applications can incorporate proprietary data.
* RAG applications can access up-to-date information.
* RAG can enhance the accuracy of LLM responses.
* RAG enables fine-grained data access control.

### RAG + Vector Search
RAG with Vector Search involves **retrieving** information using a vector database, **augmenting** the userâ€™s prompt with that information and **generating** a response based on the userâ€™s prompt and information retrieved using an LLM.

**Typical RAG Architecture (simplified)**
```sql
User Query
   â†“
Retriever â†’ Vector Database (e.g., Pinecone, FAISS, Chroma)
   â†“
Relevant Context
   â†“
Generator (LLM like GPT-5)
   â†“
Answer grounded in the retrieved data
```

**Typical RAG Architecture (detailed)**
```pgsql
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚         USER QUERY         â”‚
                 â”‚ (Question / Prompt Input)  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚       RETRIEVER        â”‚
                     â”‚ orchestrates retrieval â”‚
                     â”‚ (LangChain, LlamaIndex,â”‚
                     â”‚ Haystack, custom API)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               EMBEDDING MODEL                 â”‚
         â”‚  Converts text/query â†’ numerical vector       â”‚
         â”‚                                               â”‚
         â”‚ ğŸ”¸ Examples:                                  â”‚
         â”‚   - OpenAI: text-embedding-3-small/large      â”‚
         â”‚   - HuggingFace: all-MiniLM, bge-base         â”‚
         â”‚   - Cohere: embed-english-v3.0                â”‚
         â”‚   - Google: textembedding-gecko               â”‚
         â”‚   - Voyage, Nomic, Instructor-xl              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                VECTOR DATABASE                â”‚
         â”‚ Stores {vector, text, metadata}               â”‚
         â”‚ Performs similarity search (cosine, dot, L2)  â”‚
         â”‚                                               â”‚
         â”‚ ğŸ”¸ Examples:                                  â”‚
         â”‚   - Local: Chroma, FAISS                      â”‚
         â”‚   - Distributed: Qdrant, Milvus, Weaviate     â”‚
         â”‚   - SQL-based: pgvector (Postgres), Redis     â”‚
         â”‚   - Managed: Pinecone, Elastic, Vespa.ai      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              RETRIEVER LOGIC                  â”‚
         â”‚ Finds top-k most similar vectors              â”‚
         â”‚ Optionally re-ranks or filters metadata       â”‚
         â”‚                                               â”‚
         â”‚ ğŸ”¸ Techniques:                                â”‚
         â”‚   - Similarity Search (cosine, dot product)   â”‚
         â”‚   - MMR (Max Marginal Relevance)              â”‚
         â”‚   - Multi-Query Expansion                     â”‚
         â”‚   - Rerankers (cross-encoders, Cohere Rerank) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚          LLM (GENERATOR / REASONER)           â”‚
         â”‚ Consumes retrieved context to compose answer  â”‚
         â”‚                                               â”‚
         â”‚ ğŸ”¸ Examples:                                  â”‚
         â”‚   - GPT-4, GPT-5 (OpenAI)                     â”‚
         â”‚   - Claude 3.x (Anthropic)                    â”‚
         â”‚   - Gemini (Google)                           â”‚
         â”‚   - Mistral, Llama-3, Command R+              â”‚
         â”‚                                               â”‚
         â”‚ ğŸ”¸ Frameworks: LangChain, LlamaIndex, DSPy     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚            FINAL ANSWER TO USER               â”‚
         â”‚   Grounded, contextual, explainable output    â”‚
         â”‚   (can include citations & sources)           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

![[RAG_vs_LLM.jpg]]

---

**Synchronous RAG**
![[R_A_G.jpg]]
This only works if the AI is going to respond quickly.

* One of the important things about RAG systems, the process diagrammed above takes time.
* If you have a request that takes more than 30 seconds, then the API will time out.  APIs and HTTP are designed to be very fast.
* To deal with a process that takes a long time:
	1. Use asynchronous RAG

		**Asynchronous RAG from DataExpert.io**
		![[asynchronous_RAG.jpg]]

		The process to the right is happening in an offline process that is not on the server.  A process on the side that is not relying on HTTP.  The API can then respond quickly.

### Synchronous vs. Asynchronous RAG
* Is the user expecting an immediate response?
	* Suffer latency optimization hell and use synchronous.
* Can the user wait for feedback?
	* Use asynchronous RAG
* ==If your API response take longer than 15 seconds, you should use Asynchronous RAG!==

## 006.2.3.3 Fine Tuning

==You create your own model and you add your own data to your own model==

![[Fine_Tuning_Diagram.jpg]]

* The access pattern for fine tuning is much simpler!
* You are basically moving the complexity.  Before, the complexity was in the API layer, not it is going to be in the model and the serving layer of the model.  You get a simple access pattern, but now the model is a lot more complex.
* Good to use if most of the data you're using is not available on the internet.

* How to do fine tuning:
	* Take your custom data you're using in RAG
	* Tokenize it
	* Leverage the Trainer libraries from Transformers and your open source model of choice to get your new model! **There are a lot of parameters you can fiddle with here**

* This might sound like a lot of work but you get:
	* No need to use a vector database because all your data is already in the model
	* Small prompts and token windows for cheaper chatting.  Now all the contextual information that you were previously including in your prompt is housed in the model.  Fewer input tokens saves on cost and latency.
### RAG vs. Fine Tuning

![[RAG_vs_Fine_Tuning.jpg]]

* If your data is changing a lot, then fine tuning is not a great choice.  
* Fine tuning is done in batches.  So if your data is changing a lot, then there will always be a gap between when the model was last trained.
* RAG always has the ability to add new data very easily.