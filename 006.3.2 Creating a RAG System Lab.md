### Data preparation: Getting an external information source into a vector database

Before we can perform RAG with Vector Search, we need to get data — in this case, unstructured text data — into a vector database.

see *A Compact Guide to RAG* ![[A Compact Guide to RAG.pdf]]

![[Getting_data_into_vector_database.jpg]]

**Core steps for preparing data for RAG include the following:**
1. **Parsing the input documents**
	* JSON symbols are super annoying for the model to deal with.  Claude converted the JSON files to be more of a markdown format.
	* Whenever you are computing your embeddings, you want to get rid of as much extraneous shit as possible (no nasty JSON or HTML).
2. **Splitting the documents into chunks**
	* Chunk size can affect the output quality of a RAG application. If the chunks are too small, they may not include enough context to address the user’s query. If the chunks are too large, the LLM may fail to pull out the relevant details, focusing instead on other details included in the chunk.
3. **Embedding the text chunk**
	* After splitting the source documents into manageable chunks, use an embedding model to convert each of those chunks into a high-dimensional numerical vector.
4. **Storing and indexing the embedding models**
	* Embeddings are stored in a specialized kind of database known as a vector database, which is designed to efficiently store and search for vector data like embeddings.
	* Having a huge number of text chunks can result in slower retrieval speeds. A common approach to maintain performance is to index the embeddings with a vector index. A vector index is a mechanism, often part of a vector database, that uses various algorithms to organize and map vector embeddings in a way that optimizes search efficiency.
	* Make sure that the id for each vector is unique.  Duplicate id's will kill the RAG performance.
5. **Recording metadata**
	* Capturing metadata along with text chunks allows us to filter results based on metadata (if applicable) and provide detailed references along with the results.

### Lab Exercise

1. Add a vector database in Supabase.  Code below enables vector datatypes.
	```sql
	CREATE EXTENSION IF NOT EXISTS vector;
	```

2. Create a rag_content table.
	```sql
	CREATE TABLE rag_content (
	   id TEXT PRIMARY KEY,
	   embedding VECTOR(1536) NOT NULL, --actual vector list
	   context TEXT NOT NULL, --statistical info for the job or profile (i.e. experience level, employement type, etc)
	   user_id INTEGER NOT NULL,
	   document_type TEXT NOT NULL --Used for indexing (i.e. profiles and jobs)
	)
	```

3. Ask Claude to write a small python script that reads each file in the data folder and converts them to embeddings and loads them into `rag_content` Supabase table with this schema (above).  
	* Files were synthetic data as if scraped from LinkedIn:
		* synthetic_data_jobs.json
		* syntheticd_profiles.json
	* Output of Claude is load_embeddings.py
* JSON symbols are super annoying for the model to deal with.  Claude converted the JSON files to be more of a markdown format.
* Whenever you are computing your embeddings, you want to get rid of as much extraneous shit as possible (no nasty JSON or HTML).

4. Get embedding for the chat (user input).  Update main.py to generate embeddings.
```python
embedding_response = openai_client.embeddings.create(
            input=user_message,
            model='text-embedding-3-small'
        )
        
        query_embedding = embedding_response.data[0].embedding
```

5. Create a way to do a vector similarity match (i.e. cosine similarity).
	* Use Claude Code to write the following python script.
		"Update the chat endpoint to query `rag_content`Supabase table with the embedding and return the top 10 results using cosine distance"
		
6. Create `match_documents` function in Supabase.
	```sql
-- Create a function to search for documents using cosine similarity
-- This function takes a query embedding and returns the top N most similar documents

create or replace function match_documents(
  query_embedding vector(1536), --in rag_results code block below
  match_count int default 10 --in rag_results code block below
)

returns table (
  id text,
  context text,
  user_id int,
  document_type text,
  similarity float
)

language sql stable
as $$
  select
    id,
    context,
    user_id,
    document_type,
    1 - (embedding <=> query_embedding) as similarity
  from rag_content
  order by embedding <=> query_embedding
  limit match_count;
$$;
	```

7. We can now call the `match_documents` function with `supabase.rpc`.

**Code written by Claude in step 6 (in main.py)**
```python
# Query rag_content table with cosine distance to get top 10 results
        rag_results = supabase.rpc(
            'match_documents_by_document_type',
            {
                'query_embedding': query_embedding, # in match_documents above
                'match_count': 10, # in match_documents above
                'query_document_type': 'job' # passed into the match_documents_by_document_type function
            }
        ).execute()
```

* What will be by the `rag_results` function are the `context_items` (i.e. the jobs that actually match your prompt).

**main.py**
```python
# Extract context from RAG results
        context_items = []
        if rag_results.data:
            for item in rag_results.data:
                if item['similarity'] > .3: # threshold for similarity.  Prevents unrelated content from being returned.
                    context_items.append(item.get('context', ''))

        print(len(context_items))

        # Build context string
        rag_context = "\n\n".join(context_items) if context_items else "No relevant context found."****
```

8. Create a function in Supabase to allow the chatbot to filter based on document type (i.e. use an index for vector embeddings).  

**match_documents_by_document_type.sql**
```sql
-- Create a function to search for documents using cosine similarity
-- This function takes a query embedding and returns the top N most similar documents
-- One way to index vector embeddings.  This separates whether the LLM should return a profile or a job.

create or replace function match_documents_by_document_type(
  query_embedding vector(1536),
  query_document_type TEXT,
  match_count int default 10
)

returns table (
  id text,
  context text,
  user_id int,
  document_type text,
  similarity float
)

language sql stable
as $$
  select
    id,
    context,
    user_id,
    document_type,
    1 - (embedding <=> query_embedding) as similarity
  from rag_content
  WHERE document_type = query_document_type -- allows you to filter on the document type (index)
  order by embedding <=> query_embedding
  limit match_count;
$$;
```

9. When someone parses their resume, we want to find other profiles similar to theirs, but also other jobs related to their work history.  After the end of the resume parsing code block (in main.py), include the code below:

	```python
	parsed_resume = completion.choices[0].message.tool_calls[0].function.arguments

        # an embedding that represents the parsed resume.
        embedding_response = openai_client.embeddings.create(
            input=parsed_resume,
            model='text-embedding-3-small'
        )
        query_embedding = embedding_response.data[0].embedding
        
        jobs = query_rag_content(query_embedding, 10, 'job')
        profile = query_rag_content(query_embedding, 10, 'profile')

        job_items = []
        if jobs.data:
            for item in jobs.data:
                if item['similarity'] > .3:
                    job_items.append(item.get('context', ''))

        profile_items = []
        if profile.data:
            for item in profile.data:
                if item['similarity'] > .3:
                    profile_items.append(item.get('context', ''))

        insert_resume(json.loads(parsed_resume))

        return {"parsed_resume": parsed_resume, 'jobs': job_items, 'profiles': profile_items}
	
	```

10.  Use Claude to generate html code with the following prompt:
	" Update resume.html to display the jobs and profiles from /api/parse_resume"