[[#006.3.3.1 Relevance Options]]

## 006.3.3.1 Relevance Options
* Be mindful of what you're putting in your vector database!  Remember: garbage in, garbage out.
* Limit (or increase) the number of vectors to consider when you query your vectors.
* Re-rank your retrieval with another LLM
* Metadata filtering (preferably with partial indexes) *Zach's favorite*

### Be mindful of what you're putting in your vector database!
* Convert as much as you can to markdown (the language of LLM)
	* Maintain as much structure as you can (i.e. titles, sections, paragraphs, etc)
* Chunk smartly
	* Maintain a 10-20% overlap to not cut context
* Remove/replace smart quotes and other non-ascii characters (unless you're doing non-English LLM work)
	* If you have another language, that needs to be it's own metadata filter.  Don't mix languages in an embedding store.

* Some more standard NLP (Natural Language Processing) stumbling blocks that you ==**SHOULD NOT DO**==
	* Remove stop words (i.e. the, and, is, but)
	* Lemmatize (ex. change "run" and "runs" to "running")
		* Always have the same conjugation of your words

### Limit (or increase) the number of vectors to consider
* More vectors upon retrieval = more chance you find what you need = more noise!
* More specific questions (i.e. what is the capital of UK) = fewer needed vectors
* Broader exploratory questions (what are all the jobs available in the UK) = more vectors
* ==**Use an LLM to help determine this!**==

### Re-rank your retrieval with another LLM
* Vector match is good at finding semantic similarity but it sucks at logical things like:
	* temporal queries (give me all the data eng documents after 2022)
	* negation queries (what are all the things NOT on the list)
		* don't and not are such common words that they don't have a material affect on the vector
	* you need smaller context but hallucinations increase with small `top_k`
		* increase the `top_k`, make the retrieval more noisy, and then let them LLM decide

### When re-ranking is probably overkill
* When you have a small amount of documents that are high quality and not noisy
	* You can reduce the number of documents by filtering "ahead of time" using the metadata
* When the query is simple and specific (i.e. "What is the PTO policy?")
* If you know `top_k` is one (i.e. "What is the capital of the UK?"), then you **DO NOT** need re-ranking.
* You need extremely low latency!

### Metadata Filtering
* Metadata filtering is the **best way** to cut on the noise.
* If an LLM can determine:
	* the document
	* the category
	* the document type
	* etc.
* This will allow you to filter down on your vector space and **ONLY SEARCH RELEVANT VECTORS**.
* Better than re-ranking because it improve relevance **AND** improve latency.

## 006.3.3.2 Other types of RAG worth looking into
1. Graph RAG
2. Agentic RAG
3. Structured RAG

### Graph RAG
* Some relationships are more complex than what show up in a document (i.e. tertiary connections).
* Using tools like Neo4j and graph traversal and build a lot more context.
* Can ask "What data does this application have access to indirectly (i.e. by having access to a secondary application)?"

### Agentic RAG
* Tools can allow Agents to fetch context
	* ChatGPT does this all the time with "web search"
* This context can be really powerful because it's live and relevant!

### Structured RAG
* Text-to-SQL relies not just on data schema but also the actual shape of the data.
* Structured RAG is a form of Agentic RAG (specific to SQL databases).

## How to offload heavy AI work to a worker task
* The more valuable the task, the longer the AI is going to take to complete it!
* Your server code thrives on low latency (15 secs or less).  Users of websites expect fast responses.
* Use asynchronous queues to solve this problem!

