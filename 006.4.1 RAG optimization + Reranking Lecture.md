### Identifying Business Problems for AI
* You need an **integration (trigger) point** for the most successful AIs!
* Need something that will **allow** it to use it's capabilities
* It needs to have a *moment* and *context* to do something
* Trigger points turn into action which equals busines value

**Trigger points need outcomes/actions to be successful**
* **Web Chatbot**
	* Triggered by customer
	* Potential actions (leveraging MCP tools):
		* Low risk (definitely use LLM)
			* Explain schedule, details
			* Suggesting flights, prices
		* Medium risk (consider LLM)
			* Canceling subscription
			* Book flight for customer
			* Free admission, giveaways, etc
		* High risk (avoid LLM for now)
			* Negotiating sales
			* Handling upset customers
			* Issuing refunds

![[web_chatbot_trigger_points.jpg]]

* **CI/CD**
	* Triggered by software engineers
		* Potential actions (leveraging MCP tools):
			* Low risk (definitely use LLM)
				* Write a code review
				* Suggest missing unit test/integration tests and open PR
			* Medium risk (consider LLM)
				* Reject a pull request
			* High risk (avoid LLM for now)
				* Deploy to production

![[CI_CD_Trigger_Example.jpg]]
*PR = Pull Request*

* **CRON job**
	* Triggered by data engineers
	* Potential actions (leveraging MCP tools)
		* Low risk (definitely use LLM)
			* Raise anomalies with explanation to data engineers via email or Slack
		* Medium risk (consider LLM)
			* Manually troubleshoot a memory error
		* High risk (avoid LLM for now)
			* Unblock a data quality problem that it labels as innocuous

	![[CRON_job_Trigger_Example.jpg]]

* **Event Queue**
	* Triggered by event on site
	* Potential actions (leveraging MCP tools)
		* Low risk (definitely use LLM)
			* A user logs in for the first time in a while, send them a personalized welcome back email
		* Medium risk (consider LLM)
			* Grade users homework and update their certification progress
		* High risk (avoid LLM for now)
			* A user is doing sketchy stuff on the website, automatically label as fake and block access

	![[Event_queue_trigger_example.jpg]]

### How to improve performance of Vector search?
* Indices are available!

1. **IVFF Index** (less accuracy. amazing scalability)
	* Stands for *Inverted File with Flat quantization*
	* Use if you are writing a lot of vectors
	* Better for writing vectors
2. **HNSW Index** (more accuracy, more memory and longer write time)
	* Use for reading vectors

**IVFF vs. HNSW**
![[IVFF_vs_HNSW.jpg]]
*The big difference is going to be accuracy and insert speed.  These are inversely related.*

#### HNSW = Hierarchical Navigable Small World
* HNSW constructs a multi-layer graph where nodes represent vectors, and edges connect similar vectors, allowing nearest-neighbor searches by traversing graph edges.
![[HNSW_diagram.jpg]]

* Think about it as degrees of separation

**HNSW has Three Parameters:**
* Two at build time:
	1. `m` = how many neighbors are at each layer
	2. `ef_construction` = how aggressive at scanning
* One at query time:
	1. `ef_search` = how many candidate neighbors do you want to explore
		* As you increase `ef_search` you will get less semantically relevant results, but, as you rerank, some of those less relevant results may be the most important for the AI.

#### IVFF
* IVF uses K-means clustering to organize similar vectors into the same cluster.  Depending on the number of vectors in the database, it's typical to set the number of clusters so that, on average, there are 100 to 10,000 vectors in each cluster.  During querying, IVF finds the cluster centroids closest to the query embedding, and the vectors in these clusters become candidate neighbors.

![[IVFF_diagram.jpg]]
*For orange segment, probes is set to 5*

**IVFF has Two Parameters:
* One at build time:
	1. `lists` = how many sectors should we break the vector space into
		* If you have more shards (i.e. lists), there will be less vectors in each shard.  That will speed things up because it makes each area smaller to scan.
* One at query time:
	1. `probes` = how many of the closest sectors we should scan
		* As you increase probes, you increase the number of vectors you are scanning.

![[lists_probe_graph.jpg]]
*Inverse relationship between query speed and relevancy*

---
### How can we make our RAG more relevant?
* RAG results are less relevant because:
	* They don't have memory context
		* Need to put previous conversations in the chatbot into vectors
	* Boundary fragmentation / insufficient chunk information
	* Bad matching / too many results

#### Data processing side
* Chunks should **encapsulate an "idea" or "though"** (i.e. a complete thought)
	* If you can logically split your chunks in paragraphs
		* easy with written docs, hard with transcripts
	* For less "organized" text
		* consider overlapping chunks!
	* Consider adding "duplicative" chunks that are hierarchical
		* Document → Page → Paragraph → Sentence
			* Page can have a Document ID

#### Too many results / bad matching
* Consider adding sparse index matching (or other keyword matching)
* Reducing the search space can dramatically increase relevance by segmenting on metadata!  This is why metadata is super important for vectors.
* Re-rank via another LLM!  Can use a shittier LLM model to do the re-ranking since it already has the context. 
	* Keep in mind re-ranking is not free.  It does add latency and cost.

#### Segmenting the Data
* Helps reduce the number of vectors you are searching by searching only the relevant vectors the prompt is looking for.

![[segmenting_data_diagram.jpg]]

1. Come up with a canonical list of categories
2. Give that list of categories to both the document ingestion pipeline as well as the API layer
3. When someone submits a prompt, it figures out which category the prompt belongs to.  If it can't figure a category, it doesn't filter based on category and just does a more expense search.
4. On the left side, when you ingest documents, you use an LLM to figure out which category the document belongs to.
#### Re-ranking

![[reranking_diagram.jpg]]
